---
title: "Translation with RNN"
output:
  html_document:
    theme: paper
    toc: yes
    toc_depth: 2
    toc_float: false
editor_options: 
  chunk_output_type: console
---

```{r}
library(data.table)
library(plotly)
library(readr)
library(mxnet)
library(stringi)
library(aws.s3)

source("rnn_encode.R")
source("rnn_decode.R")
source("model_rnn.R")
source("attention.R")
source("metrics.R")
```

```{r}
# https://912166914775.signin.aws.amazon.com/console
```

### Load buckets

```{r}
#buckets <- read_rds("data/buckets_en_fr_4_24.rds")
buckets <- s3readRDS(object = "translatR/data/euro/buckets_euro_en_fr_4_24.rds", bucket = "nimus-data")
```


### Model architecture

```{r}
source_input_size = nrow(buckets$source_dic)
target_input_size = nrow(buckets$target_dic)

batch_size = 128
num_hidden = 256
num_embed = 256
seq_len = 24
num_rnn_layer = 2
num_proj_key = NULL

# encode <- rnn.graph.unroll.encode(seq_len = 2, num_rnn_layer = 1, input_size = source_input_size, num_embed = num_embed, output_last_state = F, config = "one-to-one", cell_type = "lstm", loss_output = NULL, ignore_label = 0, masking = T, num_hidden = num_hidden, dropout = 0.2, prefix = "encode_", init.state = NULL, data_name = "data", bidirectional = T, reverse_input = F)
# 
# graph.viz(encode)
```

```{r}
encode <- rnn.graph.unroll.encode(seq_len = seq_len, num_rnn_layer = num_rnn_layer, input_size = source_input_size, num_embed = num_embed, output_last_state = F, config = "one-to-one", cell_type = "lstm", loss_output = NULL, ignore_label = 0, masking = T, num_hidden = num_hidden, dropout = 0.2, prefix = "encode_", init.state = NULL, data_name = "data", bidirectional = T, reverse_input = F)
```


### Attention

Illustration of the attention mechanism. 

The function _attention_dot_ has three intrants: 

- query: last hidden output of the decoder at current timestep. 
- key: features against which a query will be performed. 
- value: the features to which the attention mechanism will attend. These are the last hidden output of the encoder in a typical RNN seq2seq. 

In a basic setting, the query = last hidden and key = value. 
There are however multiple variations possible, one of which

```{r, fig.height=10}
query = mx.symbol.Variable("query")
key = mx.symbol.Variable("key")
value = mx.symbol.Variable("value")

# query = mx.symbol.identity(query)
key = mx.symbol.identity(key, name = "key")
value = mx.symbol.identity(value, name = "value")
query_proj_weight = mx.symbol.Variable("query_proj_weight")

attn_param_graph = list(query = query, key = key, value = value,
                        query_proj_weight = query_proj_weight,
                        scaled = T, weighting = T, 
                        num_hidden = num_hidden, 
                        num_proj_key = num_proj_key)

attention = attention_dot

attention_graph = do.call(attention, args = attn_param_graph)
graph.viz(attention_graph, shape = list(query = c(num_hidden, batch_size),
                                        key = c(num_hidden, seq_len, batch_size),
                                        value = c(num_hidden, seq_len, batch_size)))
```


```{r}
attn_key_value = attn_key_create(encode = encode, num_proj_key = num_proj_key)

attn_ini = attention_ini(key = attn_key_value$key, value = attn_key_value$value)
query_proj_weight = mx.symbol.Variable("query_proj_weight")
attn_param = list(key = attn_key_value$key, value = attn_key_value$value, 
                  query_proj_weight = query_proj_weight,
                  scaled = T, weighting = T, 
                  num_hidden = num_hidden, 
                  num_proj_key = num_proj_key)

decode <- rnn.graph.unroll.decode(encode = encode, attn_ini = attn_ini, attention = attention, attn_param = attn_param, seq_len = seq_len, num_rnn_layer = num_rnn_layer, input_size = NULL, num_embed = NULL, output_last_state = F, config = "one-to-one", cell_type = "lstm", loss_output = "softmax", ignore_label = 0, masking = F, num_decode = target_input_size, num_hidden = num_hidden, dropout = 0.2, prefix = "decode_", label_name = "label")

encode$arguments
shapes = encode$infer.shape(list(data=c(seq_len, batch_size)))

decode$arguments
shapes = decode$infer.shape(list(data=c(seq_len, batch_size), label=c(seq_len, batch_size)))

shapes_out = shapes$out.shapes
shapes_arg = shapes$arg.shapes
```

### Prepare iterators 

```{r}
iter_train <- mx.io.arrayiter(data = buckets$buckets$`24`$data, label = buckets$buckets$`24`$label, batch.size = batch_size, shuffle = F)

iter_train$reset()
iter_train$iter.next()
iter_data = iter_train$value()
```



### Launch training

```{r, eval=FALSE}
# ctx <- mx.cpu()
ctx <- mx.gpu(0)

initializer <- mx.init.Xavier(rnd_type = "gaussian", factor_type = "in", magnitude = 2.5)

# optimizer <- mx.opt.create("rmsprop", learning.rate = 1e-3, gamma1 = 0.95, gamma2 = 0.95,
#                            wd = 1e-5, clip_gradient = 1, rescale.grad=1)

# optimizer <- mx.opt.create("adadelta", rho = 0.95, epsilon = 1e-8, wd = 1e-8,
#                            clip_gradient = 1, rescale.grad=1)

optimizer <- mx.opt.create("adam", learning.rate = 5e-4, beta1 = 0.9, beta2 = 0.999,
                           epsilon = 1e-8, wd = 0,
                           clip_gradient = 1, rescale.grad=1)

logger <- mx.metric.logger()
epoch.end.callback <- mx.callback.log.train.metric(period = 1)
batch.end.callback <- mx.callback.log.speedometer(batch.size = batch_size, frequency = 25)

system.time(
  model <- mx.model.buckets(symbol = decode,
                            train.data = iter_train, 
                            eval.data = NULL,
                            num.round = 1, ctx = ctx, verbose = TRUE,
                            metric = mx.metric.Perplexity, 
                            optimizer = optimizer,  initializer = initializer,
                            batch.end.callback = batch.end.callback, 
                            epoch.end.callback = epoch.end.callback)
)

mx.model.save(model = model, prefix = "models/model_euro_en_fr_rnn_1", iteration = 0)

```


## Inference

```{r}
model <- mx.model.load(prefix = "models/model_euro_en_fr_rnn_1", iteration = 0)
source_dic <- buckets$source_dic
target_dic <- buckets$target_dic
setkeyv(source_dic, "word")
setkeyv(target_dic, "word_id")
ctx <- mx.cpu()

infer_helper <- function(infer_seq, model, source_dic, target_dic) {
  
  infer_seq <- paste("<BOS>", infer_seq, "<EOS>")
  infer_seq <- stri_split_boundaries(infer_seq, type = "word", 
                                     skip_word_none = T, 
                                     skip_word_number = F, 
                                     simplify = T)
  
  infer_seq <- data.table(t(infer_seq))
  infer_dt <- source_dic[infer_seq]
  
  infer_mat <- matrix(0, nrow = 24, ncol = 1)
  infer_mat[1:length(infer_dt$word_id), 1] <- infer_dt$word_id
  
  iter_infer <- mx.io.arrayiter(data = infer_mat, label = infer_mat, batch.size = 1, shuffle = F)
  
  infer_pred <- mx.infer.rnn(infer.data = iter_infer, model = model, ctx = ctx)
  dim(infer_pred)
  
  infer_nd <- mx.nd.array(infer_pred)
  infer_max <- as.array(mx.nd.argmax(infer_nd, axis = 1))
  
  translation <- target_dic[infer_max+1]
  return(translation)
}

infer_helper(infer_seq = "tomorrow i want to talk about law in the parliament",
             model = model, source_dic = source_dic, target_dic = target_dic)

infer_helper(infer_seq = "i am minister",
             model = model, source_dic = source_dic, target_dic = target_dic)

infer_helper(infer_seq = "the union and the government",
             model = model, source_dic = source_dic, target_dic = target_dic)

```

