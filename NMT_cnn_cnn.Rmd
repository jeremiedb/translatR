---
title: "Fully convolutional translation - In Dev"
output:
  html_document:
    theme: paper
    toc: yes
    toc_depth: 2
    toc_float: false
editor_options: 
  chunk_output_type: console
---

```{r}
library(data.table)
library(plotly)
library(readr)
library(mxnet)
library(stringi)
library(aws.s3)

source("R/RNN_cells.R")
source("R/encoder-cnn.R")
source("R/decoder-cnn.R")
source("R/model_rnn.R")
source("R/attention.R")
source("R/metrics.R")
```

```{r}
# https://912166914775.signin.aws.amazon.com/console
```

### Load buckets

```{r}
buckets <- read_rds("data/buckets_wmt15_en_fr_8_64_v2.rds")
# buckets <- s3readRDS(object = "translatR/data/euro/buckets_euro_en_fr_10_50.rds", bucket = "nimus-data")
```


### Model architecture

```{r}
source_input_size = nrow(buckets$source_dic)
target_input_size = nrow(buckets$target_dic)

batch_size = 256
num_embed = 512
num_hidden = 512
query_key_size = 512
seq_len = 64
num_rnn_layer = 4
num_proj_key = NULL
```

```{r}
encode <- conv.graph.encode(input_size = source_input_size, num_embed = num_embed, loss_output = NULL, ignore_label = -1, masking = T, num_hidden = num_hidden, dropout = 0.1, prefix = "encode_", data_name = "data", label_name = "dummy1")
```


### Attention

Illustration of the attention mechanism. 

The function _attention_dot_ has three intrants: 

- query: last hidden output of the decoder at current timestep. 
- key: features against which a query will be performed. 
- value: the features to which the attention mechanism will attend. These are the last hidden output of the encoder in a typical RNN seq2seq. 

In a basic setting, the query = last hidden and key = value. 
There are however multiple variations possible, one of which

```{r, fig.height=10}
query = mx.symbol.Variable("query")
query = mx.symbol.identity(query, name = "query")
value = mx.symbol.Variable("value")

# attn <- attn_mlp(value = value, query_key_size = num_hidden, scale = T)
attn <- attn_dot(value = value, query_key_size = num_hidden, scale = T)

init <- attn$init()
attend <- attn$attend

attention <- attend(query = query, key = init$key, value = init$value, attn_init = init)

# graph.viz(attention)
graph.viz(attention, shape = list(query = c(num_hidden, batch_size),
                                  value = c(num_hidden, seq_len, batch_size)))

```


# Attention

Attention functions return two functions:

- an init() function to be called following the definition of the encoder so as to initialize the query, key and value components. 
- attend() function to be called each time an attention query is performed during decoding. 

```{r}
# attn <- attn_dot(value = encode, query_key_size = num_hidden, scale = T)
attn <- attn_dot(value = encode, query_key_size = query_key_size, scale = T, prefix = "attn")

decode <- conv.graph.decode(encode = encode, num_decode = target_input_size , num_embed = num_embed, loss_output = "softmax", ignore_label = 0, masking = F, num_hidden = num_hidden, dropout = 0.1, prefix = "decode_", label_name = "label", batch_size = batch_size)

# # no masking - handled implicitly thourgh the ignore label of softmax
# decode_teacher <- deco(mode = "teacher", attn = attn, seq_len = seq_len, num_rnn_layer = num_rnn_layer, input_size = target_input_size, num_embed = num_embed, output_last_state = F, cell_type = "lstm", loss_output = "softmax", ignore_label = 0, masking = F, num_decode = target_input_size, num_hidden = num_hidden, dropout = 0.1, prefix = "decode_", label_name = "label")
# 
# decode_argmax <- rnn.graph.unroll.decode(mode = "argmax", attn = attn, seq_len = seq_len, num_rnn_layer = num_rnn_layer, input_size = target_input_size, num_embed = num_embed, output_last_state = F, cell_type = "lstm", loss_output = "softmax", ignore_label = 0, masking = F, num_decode = target_input_size, num_hidden = num_hidden, dropout = 0.1, prefix = "decode_", label_name = "label")

# decode_sample <- rnn.graph.unroll.decode(mode = "sample", attn = attn, seq_len = seq_len, num_rnn_layer = num_rnn_layer, input_size = target_input_size, num_embed = num_embed, output_last_state = F, cell_type = "lstm", loss_output = "softmax", ignore_label = 0, masking = F, num_decode = target_input_size, num_hidden = num_hidden, dropout = 0.05, prefix = "decode_", label_name = "label")

# graph.viz(encode)
encode$arguments
shapes = encode$infer.shape(list(data=c(seq_len, batch_size)))

decode$arguments
shapes <- decode$infer.shape(list(data=c(seq_len, batch_size), label=c(seq_len, batch_size)))

shapes_out = shapes$out.shapes
shapes_arg = shapes$arg.shapes

# validate that argmax decoder has same arguments than one used for training
# table(arguments(decode_argmax) == arguments(decode_teacher))
```

### Prepare iterators 

```{r}

idx_tot <- sample(ncol(buckets$buckets$`64`$data), replace = F)
idx_train <- idx_tot[1:as.integer(0.99 * length(idx_tot))]
idx_eval <- setdiff(idx_tot, idx_train)

mx.set.seed(44)
iter_train <- mx.io.arrayiter(data = buckets$buckets$`64`$data[, idx_train], label = buckets$buckets$`64`$label[, idx_train], batch.size = batch_size, shuffle = T)
iter_eval <- mx.io.arrayiter(data = buckets$buckets$`64`$data[, idx_eval], label = buckets$buckets$`64`$label[, idx_eval], batch.size = batch_size, shuffle = T)

```

```{r}
# perplexity jumps in the last records - to be investigated
# 1 248 000 - 1 251 200
# potential buggy records between 1 270 400 and 1 272 000 and 1 280 000 and 1 283 200
# dim(buckets$buckets$`50`$data[, -(1248000:1272000)])
# dim(buckets$buckets$`50`$label[, -(1248000:1272000)])
# iter_train <- mx.io.arrayiter(data = buckets$buckets$`50`$data[, -(1248000:1272000)], label = buckets$buckets$`50`$label[, -(1248000:1272000)], batch.size = batch_size, shuffle = F)

# iter_train <- mx.io.arrayiter(data = buckets$buckets$`50`$data[, 1:1000000], label = buckets$buckets$`50`$label[, 1:1000000], batch.size = batch_size, shuffle = T)
# 
# iter_train$reset()
# iter_train$iter.next()
# iter_data = iter_train$value()
# dim(iter_data$label)
```


### Launch training

```{r, eval=FALSE}
# ctx <- mx.cpu()
ctx <- mx.gpu(0)

initializer <- mx.init.Xavier(rnd_type = "uniform", factor_type = "in", magnitude = 2.5)

# optimizer <- mx.opt.create("rmsprop", learning.rate = 1e-3, gamma1 = 0.95, gamma2 = 0.95,
#                            wd = 1e-5, clip_gradient = 1, rescale.grad=1)

# optimizer <- mx.opt.create("adadelta", rho = 0.95, epsilon = 1e-8, wd = 1e-8,
#                            clip_gradient = 1, rescale.grad=1)

lr_scheduler <- mx.lr_scheduler.FactorScheduler(step = 5000, 
                                                factor_val = 0.9, 
                                                stop_factor_lr = 5e-5)
optimizer <- mx.opt.create("adam", learning.rate = 5e-4, beta1 = 0.9, beta2 = 0.999,
                           epsilon = 1e-8, wd = 1e-8,
                           clip_gradient = 1, rescale.grad = 1, lr_scheduler = lr_scheduler)

logger <- mx.metric.logger()
epoch.end.callback <- mx.callback.log.train.metric(period = 1)
batch.end.callback <- mx.callback.log.speedometer(batch.size = batch_size, frequency = 100)

model <- mx.model.buckets(symbol = decode,
                          train.data = iter_train,
                          eval.data = iter_eval,
                          num.round = 2, ctx = ctx, verbose = TRUE,
                          metric = mx.metric.Perplexity,
                          optimizer = optimizer,  initializer = initializer,
                          batch.end.callback = batch.end.callback,
                          epoch.end.callback = epoch.end.callback)

mx.model.save(model = model, prefix = "models/model_wmt15_en_fr_cnn_cnn_teacher_v2", iteration = 2)
mx.symbol.save(symbol = decode_argmax, filename = "models/model_wmt15_en_fr_cnn_cnn_argmax_v2.json")
```


## Inference

```{r}
model <- mx.model.load(prefix = "models/model_wmt15_en_fr_cnn_rnn_teacher_v2", iteration = 12)
sym_infer <- mx.symbol.load(file.name = "models/model_wmt15_en_fr_cnn_rnn_argmax_v2.json")

source_dic <- buckets$source_dic
target_dic <- buckets$target_dic
target_dic[, word_id_char := as.character(word_id)]
setkeyv(source_dic, "word")
setkeyv(target_dic, "word_id_char")
ctx <- mx.cpu()

model_infer <- list(symbol = sym_infer, arg.params = model$arg.params, aux.params = model$aux.params)
model_infer <- structure(model_infer, class="MXFeedForwardModel")

infer_helper <- function(infer_seq, model, source_dic, target_dic) {
  
  infer_seq <- gsub(pattern = "([[:punct:]])", replacement = " \\1 ", x = infer_seq)
  infer_seq <- paste("<BOS>", infer_seq, "<EOS>")
  infer_seq <- unlist(stri_split(str = infer_seq, regex = "\\s+"))
  
  infer_seq <- data.table(infer_seq)
  infer_dt <- source_dic[infer_seq]
  
  zeros <- matrix(0, nrow = 64, ncol = 1)
  data <- zeros
  data[1:length(infer_dt$word_id), 1] <- infer_dt$word_id
  
  iter_infer <- mx.io.arrayiter(data = data, label = zeros, batch.size = 1, shuffle = F)
  
  infer_pred <- mx.infer.rnn(infer.data = iter_infer, model = model, ctx = ctx)
  # dim(infer_pred)
  
  infer_nd <- mx.nd.array(infer_pred)
  infer_max <- as.array(mx.nd.argmax(infer_nd, axis = 1))
  
  translation <- target_dic[as.character(infer_max)]
  return(list(data = data, translation = translation))
}

infer_helper(infer_seq = "Tomorrow I want to talk about the laws in the parliament.",
             model = model_infer, source_dic = source_dic, target_dic = target_dic)

infer_helper(infer_seq = "i am minister",
             model = model_infer, source_dic = source_dic, target_dic = target_dic)

infer_helper(infer_seq = "the union and the government",
             model = model_infer, source_dic = source_dic, target_dic = target_dic)

infer_helper(infer_seq = "That's what it is.",
             model = model_infer, source_dic = source_dic, target_dic = target_dic)

infer_helper(infer_seq = "I'd love to learn French!",
             model = model_infer, source_dic = source_dic, target_dic = target_dic)

```

## BLEU score

```{r}
ctx <- mx.gpu()
bucket_test <- read_rds("data/buckets_wmt14_eval_en_fr_64_v2.rds")
batch_size <- 128
seq_len <- 64
test_size <- ncol(bucket_test$buckets[[as.character(seq_len)]]$data)

model <- mx.model.load(prefix = "models/model_wmt15_en_fr_cnn_rnn_teacher_v2", iteration = 12)
sym_infer <- mx.symbol.load(file.name = "models/model_wmt15_en_fr_cnn_rnn_argmax_v2.json")

sym_infer$arguments

source_dic <- bucket_test$source_dic
target_dic <- bucket_test$target_dic
target_dic[, word_id_char := as.character(word_id)]
setkeyv(source_dic, "word")
setkeyv(target_dic, "word_id_char")

model_infer <- list(symbol = sym_infer, arg.params = model$arg.params, aux.params = model$aux.params)
model_infer <- structure(model_infer, class="MXFeedForwardModel")

i <- 1
trans_tot <- NULL

for (i in seq_len(ceiling(test_size / batch_size))) {
  
  gc()
  idx <- ((i-1) * batch_size + 1):min(test_size, (i*batch_size)) 
  
  iter_test <- mx.io.arrayiter(data = bucket_test$buckets[[as.character(seq_len)]]$data[, idx], label = bucket_test$buckets[[as.character(seq_len)]]$label[, idx], batch.size = length(idx), shuffle = F)
  
  infer_pred <- mx.infer.rnn(infer.data = iter_test, model = model_infer, ctx = ctx)
  infer_nd <- mx.nd.array(infer_pred)
  infer_max <- as.array(mx.nd.argmax(infer_nd, axis = 1))
  translation <- target_dic[as.character(infer_max)]
  translation[, id := rep(1:seq_len, times = length(idx))]
  translation[, seq_id := rep(idx, each = seq_len)]
  
  translation[, EOS_id := which.max(word == "<EOS>"), by = seq_id]
  translation[, EOS_id := ifelse(EOS_id > 1, EOS_id, 999L), by = seq_id]
  translation[, .KEEP := id > 1 & id < EOS_id & word != "<UNKNOWN>"]
  
  trans_agg <- translation[, .(agg = paste0(word, collapse = " ")), by = c("seq_id", ".KEEP")]
  trans_tot <- rbind(trans_tot, trans_agg)
  
}

trans_tot <- trans_tot[order(seq_id, .KEEP)]
trans_tot <- trans_tot[, .(agg = last(agg)), by = seq_id]

# Punctuation post process
translation <- trans_tot$agg
translation <- gsub(pattern = '\\s+', replacement = ' ', x = translation)
translation <- gsub(pattern = "\\s(\\.)", replacement = "\\1", x = translation)
translation <- gsub(pattern = "\\s(,)", replacement = "\\1", x = translation)
translation <- gsub(pattern = "\\s(;)", replacement = "\\1", x = translation)
translation <- gsub(pattern = "\\s(\\?)", replacement = "\\1", x = translation)
translation <- gsub(pattern = "\\s(\\!)", replacement = "\\1", x = translation)
translation <- gsub(pattern = "\\s(')\\s", replacement = "\\1", x = translation)
# translation <- gsub(pattern = "\\s(-)\\s", replacement = "\\1", x = translation)
translation <- gsub(pattern = " \\( ", replacement = " \\(", x = translation)
translation <- gsub(pattern = " \\) ", replacement = "\\) ", x = translation)
translation <- gsub(pattern = "<UNKNOWN>", replacement = " ", x = translation)

write_lines(translation, path = "data/translation/wmt14_en_fr_cnn_rnn_v2.txt")
```

